{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)\n",
    "\n",
    "# Big Data Management - Assignment 2\n",
    "## Graph Databases\n",
    "\n",
    "### by Natalia Bertr√°n, Clarice Mottet\n",
    "\n",
    "0. **[Part 0: Set Up and Create Sample Data](#part0)**\n",
    "- **Objective**: Initialize programming environment.\n",
    "  - Create sample data as defined in the previous problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part0'>Part 0: Set Up and Create Sample Data</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize programming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import py2neo\n",
    "from py2neo import Graph\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import sys\n",
    "sys.path.append('Inputs/')\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Create sample data as defined in the homework assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables for creating fake data - increase if needed\n",
    "\n",
    "N_PAPER = 250\n",
    "N_AUTHOR = 50\n",
    "N_EDITION = 10\n",
    "N_CONFERENCE = 2\n",
    "N_VOLUME = 10\n",
    "N_JOURNAL = 4\n",
    "N_YEAR = 5\n",
    "N_KEYWORD = 100\n",
    "N_UNIVERSITIES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fake node data\n",
    "\n",
    "dict_nodes = {}\n",
    "fake = Faker(['en_US'])\n",
    "\n",
    "node_type = 'paper'\n",
    "size = N_PAPER\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set([fake.paragraph(nb_sentences=1) for iter_ in range(int(size*1.2))]))\n",
    "list_data_ = list(set([fake.paragraph(nb_sentences=5) for iter_ in range(int(size*1.2))]))\n",
    "list_titles = []\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'title':list_data[iter_], 'abstract':list_data_[iter_]})\n",
    "    list_titles.append(list_data[iter_])\n",
    "\n",
    "node_type = 'author'\n",
    "size = N_AUTHOR\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set([fake.name() for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'name':list_data[iter_]})\n",
    "\n",
    "node_type = 'edition'\n",
    "size = N_EDITION\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set([fake.city() for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'city':list_data[iter_]})\n",
    "\n",
    "node_type = 'conference'\n",
    "size = N_CONFERENCE\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set(['Conference of '+fake.sentence(nb_words=4) for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'conference':list_data[iter_]})\n",
    "\n",
    "node_type = 'volume'\n",
    "size = N_VOLUME\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set([str(iter_) for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'volume':list_data[iter_]})\n",
    "\n",
    "node_type = 'journal'\n",
    "size = N_JOURNAL\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set(['Journal of '+fake.sentence(nb_words=4) for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'journal':list_data[iter_]})\n",
    "\n",
    "node_type = 'year'\n",
    "size = N_YEAR\n",
    "dict_nodes[node_type] = []\n",
    "list_data = [iter_ for iter_ in range(2018,2018+N_YEAR)]\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'year':list_data[iter_]})\n",
    "\n",
    "node_type = 'keyword'\n",
    "size = N_KEYWORD\n",
    "dict_nodes[node_type] = []\n",
    "str_titles = ' '.join(list_titles)\n",
    "str_titles = str_titles.replace('.','').lower()\n",
    "df_list = pd.DataFrame({'word':str_titles.split()})\n",
    "df_list['count'] = 1\n",
    "df_list['count'] = df_list.groupby(by = ['word'])['count'].transform('sum')\n",
    "df_list.drop_duplicates(subset = ['word'], inplace = True)\n",
    "df_list.sort_values(by = ['count'], ascending = [False], inplace = True)\n",
    "df_list = df_list[df_list['word'].str.len() > 5]\n",
    "keyword_list = list(df_list['word'])\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'keyword':keyword_list[iter_]})\n",
    "\n",
    "node_type = 'university'\n",
    "size = N_UNIVERSITIES\n",
    "dict_nodes[node_type] = []\n",
    "list_data = list(set(['University of '+fake.sentence(nb_words=2) for iter_ in range(int(size*1.2))]))\n",
    "for iter_ in range(size):\n",
    "    dict_nodes[node_type].append({'university':list_data[iter_]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fake edge connections\n",
    "\n",
    "dict_edges = {}\n",
    "\n",
    "dict_edges['e2c_appears_in'] = []\n",
    "list_designate = [random.randint(0,N_CONFERENCE-1) for iter_ in range(N_EDITION)]\n",
    "for edition_index_, conference_designation_ in enumerate(list_designate):\n",
    "    dict_edges['e2c_appears_in'].append({'edition':dict_nodes['edition'][edition_index_]['city'], 'conference':dict_nodes['conference'][conference_designation_]['conference']})\n",
    "\n",
    "dict_edges['v2j_appears_in'] = []\n",
    "list_designate = [random.randint(0,N_JOURNAL-1) for iter_ in range(N_JOURNAL)]\n",
    "for volume_index_, journal_designation_ in enumerate(list_designate):\n",
    "    dict_edges['v2j_appears_in'].append({'volume':dict_nodes['volume'][volume_index_]['volume'], 'journal':dict_nodes['journal'][journal_designation_]['journal']})\n",
    "\n",
    "dict_edges['e2y_occured_in'] = []\n",
    "for edition_index_ in range(N_EDITION):\n",
    "    year_designation_ = edition_index_ % N_YEAR\n",
    "    dict_edges['e2y_occured_in'].append({'edition':dict_nodes['edition'][edition_index_]['city'], 'year':dict_nodes['year'][year_designation_]['year']})\n",
    "\n",
    "dict_edges['v2y_occured_in'] = []\n",
    "for volume_index_ in range(N_VOLUME):\n",
    "    year_designation_ = volume_index_ % N_YEAR\n",
    "    dict_edges['v2y_occured_in'].append({'volume':dict_nodes['volume'][volume_index_]['volume'], 'year':dict_nodes['year'][year_designation_]['year']})\n",
    "\n",
    "dict_edges['p2k_has'] = []\n",
    "for paper_index_ in range(N_PAPER):\n",
    "    paper_title = dict_nodes['paper'][paper_index_]['title']\n",
    "    for keyword_index_ in range(N_KEYWORD):\n",
    "        keyword = dict_nodes['keyword'][keyword_index_]['keyword']\n",
    "        if keyword in paper_title:\n",
    "            dict_edges['p2k_has'].append({'title':paper_title, 'keyword':keyword})\n",
    "\n",
    "dict_edges['p2e_published_in'] = []\n",
    "dict_edges['p2v_published_in'] = []\n",
    "list_designate = [random.randint(0,1) for iter_ in range(N_PAPER)]\n",
    "for paper_index_ in range(N_PAPER):\n",
    "    paper_title = dict_nodes['paper'][paper_index_]['title']\n",
    "    if list_designate[paper_index_] == 0:\n",
    "        edition_designation_ = paper_index_ % N_EDITION\n",
    "        dict_edges['p2e_published_in'].append({'title':paper_title, 'edition':dict_nodes['edition'][edition_designation_]['city']})\n",
    "    else:\n",
    "        volume_designation_ = paper_index_ % N_VOLUME\n",
    "        dict_edges['p2v_published_in'].append({'title':paper_title, 'volume':dict_nodes['volume'][volume_designation_]['volume']})\n",
    "\n",
    "dict_edges['p2p_cites'] = []\n",
    "list_n_cites = [random.randint(3,12) for iter_ in range(N_PAPER)]\n",
    "for paper_index_ in range(N_PAPER):\n",
    "    paper_title = dict_nodes['paper'][paper_index_]['title']\n",
    "    list_cites = list(set([random.randint(0,N_PAPER-1) for iter_ in range(list_n_cites[paper_index_])]))\n",
    "    if paper_index_ in list_cites:\n",
    "        list_cites.remove(paper_index_)\n",
    "    for paper_ref_ in list_cites:\n",
    "        paper_ref_title = dict_nodes['paper'][paper_ref_]['title']\n",
    "        dict_edges['p2p_cites'].append({'title':paper_title, 'title_cited':paper_ref_title})\n",
    "\n",
    "dict_edges['a2p_wrote'] = []\n",
    "dict_edges['a2p_reviewed'] = []\n",
    "dict_edges['a2p_reviewed_detail'] = []\n",
    "\n",
    "for paper_index_ in range(N_PAPER):\n",
    "    if paper_index_ < N_AUTHOR:\n",
    "        author_index_ = paper_index_\n",
    "    else:\n",
    "        author_index_ = random.randint(0, N_AUTHOR-1)\n",
    "\n",
    "    #assign writer to paper\n",
    "    dict_edges['a2p_wrote'].append({'name':dict_nodes['author'][author_index_]['name'], 'title':dict_nodes['paper'][paper_index_]['title']})\n",
    "\n",
    "    #assign reviewers (cannot be writer)\n",
    "    review_author_list_  = list(set([random.randint(0, N_AUTHOR-1) for i_ in range(N_AUTHOR)]))\n",
    "    if author_index_ in review_author_list_:\n",
    "        review_author_list_.remove(author_index_)\n",
    "\n",
    "    n_reviews = random.randint(1,10)\n",
    "    for iter_ in range(n_reviews):\n",
    "        dict_edges['a2p_reviewed'].append({'name':dict_nodes['author'][review_author_list_[iter_]]['name'], 'title':dict_nodes['paper'][paper_index_]['title']})\n",
    "\n",
    "        acceptance_status_options = [1, 0]\n",
    "        probabilities = [0.9, 0.1]\n",
    "        acceptance_status = random.choices(acceptance_status_options, weights=probabilities, k=1)[0]\n",
    "        dict_edges['a2p_reviewed_detail'].append({'name':dict_nodes['author'][review_author_list_[iter_]]['name'], 'title':dict_nodes['paper'][paper_index_]['title'], 'review':fake.paragraph(nb_sentences=5), 'acceptance_status': acceptance_status})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_edges['a2u_affiliation'] = []\n",
    "for author_index_ in range(N_AUTHOR):\n",
    "    university_index_ = random.randint(0, N_UNIVERSITIES-1)\n",
    "    dict_edges['a2u_affiliation'].append({'name':dict_nodes['author'][author_index_]['name'], 'university':dict_nodes['university'][university_index_]['university']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the fake data\n",
    "\n",
    "file = 'node.json'\n",
    "nodes_out = f\"Inputs/zjson_files/{file}\"\n",
    "with open(nodes_out, 'w') as f:\n",
    "    json.dump(dict_nodes, f, indent=2)\n",
    "\n",
    "file = 'edges.json'\n",
    "edges_out = f\"Inputs/zjson_files/{file}\"\n",
    "with open(edges_out, 'w') as f:\n",
    "    json.dump(dict_edges, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import node and edge dictionary back in\n",
    "file = 'node.json'\n",
    "nodes_out = f\"Inputs/zjson_files/{file}\"\n",
    "with open(nodes_out, 'r') as file:\n",
    "    dict_nodes = json.load(file)\n",
    "\n",
    "file = 'edges.json'\n",
    "edges_out = f\"Inputs/zjson_files/{file}\"\n",
    "with open(edges_out, 'r') as file:\n",
    "    dict_edges = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export out the node data as CSV individually\n",
    "\n",
    "for node_type in dict_nodes.keys():\n",
    "    list_all = []\n",
    "    for iter_ in range(len(dict_nodes[node_type])):\n",
    "        dict_ = dict_nodes[node_type][iter_]\n",
    "        df_ = pd.DataFrame(dict_, index = [iter_])\n",
    "        list_all.append(df_)\n",
    "    file_out = f\"Inputs/nodes/{node_type}.csv\"\n",
    "    df_all = pd.concat(list_all, ignore_index = True)\n",
    "    df_all.to_csv(file_out, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export out the edge data as CSV individually\n",
    "\n",
    "for edge_type in dict_edges.keys():\n",
    "    list_all = []\n",
    "    for iter_ in range(len(dict_edges[edge_type])):\n",
    "        dict_ = dict_edges[edge_type][iter_]\n",
    "        df_ = pd.DataFrame(dict_, index = [iter_])\n",
    "        list_all.append(df_)\n",
    "    file_out = f\"Inputs/edges/{edge_type}.csv\"\n",
    "    df_all = pd.concat(list_all, ignore_index = True)\n",
    "    df_all.to_csv(file_out, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
